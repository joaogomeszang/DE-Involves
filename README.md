
Meu perfil Github certo: https://github.com/joaozang (Estou recuperando ele :disappointed:)

# **Projeto de ETL com Pentaho Data Integration (PDI) â€” Modelo Dimensional**

Este projeto apresenta a construÃ§Ã£o de um processo ETL utilizando **Pentaho Data Integration (PDI)** para ingestÃ£o, transformaÃ§Ã£o e carga de dados referentes a coletas realizadas em pontos de venda, com base no dataset `DATASET_TESTE_DE.csv`.
AlÃ©m disso, inclui a criaÃ§Ã£o de um **modelo dimensional** no PostgreSQL e um **job principal** orquestrando todas as transformaÃ§Ãµes.

*Optei por manter todas as tabelas de fato em granularidade diÃ¡ria, para garantir que o modelo preserve a granularidade original do dataset mantenha o maior nÃ­vel de flexibilidade analÃ­tica possÃ­vel evitando perda de informaÃ§Ã£o e aumentando a reutilizaÃ§Ã£o das tabelas fato. Optei tambÃ©m por nÃ£o filtrar especificamente para Setembro/2020 dentro das tabelas fato, porque a fato deve sempre permanecer genÃ©rica e nÃ£o filtrada, mantendo as boas prÃ¡ticas de DW*

---

## **ğŸ“¦ Requisitos MÃ­nimos**

### **1. Docker**

* Docker 20+
* Docker Compose (opcional)

### **2. Pentaho Data Integration (PDI)**

* VersÃ£o recomendada: **8.3, 9.1, 9.3 ou 10.x**
* Sistema operacional: Windows, Linux ou MacOS

---

## **ğŸ˜ Subindo o Banco de Dados com Docker**

Navegue atÃ© a raiz do projeto e rode os comandos abaixo:

```bash
docker build -t meu_banco .
docker run --name pg_container -p 5432:5432 -d meu_banco
docker start pg_container

```

ApÃ³s subir o banco, pode acessar com:

```bash
 docker exec -it pg_container psql -U meu_usuario -d meu_banco
```
Ao acessar o PostgreSQL, copie e cole o conteÃºdo do arquivo init.sql que se encontra na raÃ­z do projeto.
Por Ãºltimo, ao inicializar o Pentaho, colocar as seguintes credenciais nas Table Output das Transformations QuestÃ£o 8, QuestÃ£o 9 e QuestÃ£o 10:

POSTGRES_USER=meu_usuario
POSTGRES_PASSWORD=minha_senha
POSTGRES_DB=meu_banco
Porta 5432

---

## **ğŸ—„ï¸ Estrutura do Modelo Dimensional**

As tabelas criadas no banco seguem este layout:

### **DimensÃµes**

* `dim_calendario (data_ref, mes, ano)`
* `dim_pdv (id_ponto_venda, nome_ponto_venda, perfil_ponto_venda)`
* `dim_linha_produto (id_linha_produto, nome_linha_produto, marca_linha_produto)`

### **Fatos**

* `ft_disponibilidade (id_ponto_venda, id_linha_produto, data_ref, quantidade)`
* `ft_disponibilidade_agregada (id_ponto_venda, data_ref, quantidade)`
* `ft_ponto_extra (id_ponto_venda, id_linha_produto, data_ref, soma_pontos)`
* `ft_ponto_extra_agregada (id_ponto_venda, data_ref, soma_pontos)`

---

## **ğŸ”§ Metodologia (ETL)**

O projeto utiliza **Pentaho Data Integration** seguindo as instruÃ§Ãµes:

### **ğŸ“Œ Job Principal**

Foi criado um *job* chamado **Job1.kjb**, responsÃ¡vel por Executar as transformaÃ§Ãµes das questÃµes 8, 9 e 10

A estrutura do job Ã©:

```
Job1
 â”œâ”€â”€ transformaÃ§Ã£o_dimensoes (questÃ£o 8)
 â”œâ”€â”€ transformaÃ§Ã£o_fato_disponibilidade (questÃ£o 9)
 â”œâ”€â”€ transformaÃ§Ã£o_fato_ponto_extra (questÃ£o 10)
```

---

## **ğŸ“„ DescriÃ§Ã£o das QuestÃµes e Metodologia**

### **ğŸ”¹ QuestÃ£o 8 â€“ DimensÃµes**

TransformaÃ§Ã£o responsÃ¡vel por:

* Ler o dataset `DATASET_TESTE_DE.csv`
* Tratar datas (data_ref, mÃªs, ano)
* Popular:

  * `dim_calendario`
  * `dim_pdv`
  * `dim_linha_produto`
* Evitar duplicidades 

---

### **ğŸ”¹ QuestÃ£o 9 â€“ Fato Disponibilidade**

Regras aplicadas:

* Filtrar apenas registros com `TIPO_COLETA = 'Disponibilidade'`
* Contar ocorrÃªncias onde `VALOR = 'SIM'`
* Popular:

  * **ft_disponibilidade**
  * **ft_disponibilidade_agregada**

---

### **ğŸ”¹ QuestÃ£o 10 â€“ Fato Ponto Extra**

Regras aplicadas:

* Filtrar registros com `TIPO_COLETA = 'Ponto Extra'`
* Somar valores numÃ©ricos da coluna `VALOR`
* Popular:

  * **ft_ponto_extra**
  * **ft_ponto_extra_agregada**

---

## **ğŸ“Š Resultados (prints)**
<img width="1283" height="909" alt="image" src="https://github.com/user-attachments/assets/b5f82663-425c-4783-b4df-575689b9331a" />
<img width="1214" height="901" alt="image" src="https://github.com/user-attachments/assets/4243938c-e908-44a2-9e05-a74df131769e" />
<img width="1298" height="884" alt="image" src="https://github.com/user-attachments/assets/6dcf9e70-453a-4558-b0fd-a7652f023abd" />
<img width="773" height="905" alt="image" src="https://github.com/user-attachments/assets/e872c152-df1e-4439-859b-cc1b4b16ee0e" />
<img width="511" height="836" alt="image" src="https://github.com/user-attachments/assets/7a9677ad-fba6-4b80-b9d1-a2b6cb6e9696" />
<img width="636" height="832" alt="image" src="https://github.com/user-attachments/assets/640c48c4-b6f1-4be0-82ec-9a65025af5c4" />
<img width="602" height="835" alt="image" src="https://github.com/user-attachments/assets/6f4266e6-49a2-4dd5-a177-bbe9239c7520" />
<img width="667" height="839" alt="image" src="https://github.com/user-attachments/assets/9f566a8f-c623-4af7-859d-ae2fbc3b876c" />
<img width="513" height="824" alt="image" src="https://github.com/user-attachments/assets/25f7de78-d65a-49e6-8e33-1220d47f4886" />
<img width="698" height="829" alt="image" src="https://github.com/user-attachments/assets/faee4e97-6a3a-4472-92f0-c903f47c7f8a" />
<img width="519" height="835" alt="image" src="https://github.com/user-attachments/assets/0cb56ba1-2cbf-498e-a99c-bb94760f60b5" />









